{"cells":[{"attachments":{},"cell_type":"markdown","id":"90e7e4a4-a3e4-4030-9f85-b1c30afd3247","metadata":{},"source":["# <b><center>Beginner's Guide to Feature Selection in Python</center></b>\n","\n","**Learn about the basics of feature selection and how to implement and investigate various feature selection techniques in Python.**"]},{"attachments":{},"cell_type":"markdown","id":"2102f4c5","metadata":{},"source":["## Understanding the importance of feature selection\n","\n","The importance of feature selection can best be recognized when you are dealing with a dataset that contains a vast number of features. This type of dataset is often referred to as a _high dimensional_ dataset. Now, with this high dimensionality, comes a lot of problems such as - this high dimensionality will significantly increase the training time of your machine learning model, it can make your model very complicated which in turn may lead to Overfitting.\n","\n","Often in a high dimensional feature set, there remain several features which are redundant meaning these features are nothing but extensions of the other essential features. These redundant features do not effectively contribute to the model training as well. So, clearly, there is a need to extract the most important and the most relevant features for a dataset in order to get the most effective predictive modeling performance.\n","\n","*\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\"*\n","\n","- <a href = \"http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf\">An Introduction to Variable and Feature Selection</a>\n","\n","Now let's understand the difference between ***dimensionality reduction*** and feature selection.\n","\n","Sometimes, feature selection is mistaken with dimensionality reduction. But they are different. Feature selection is different from dimensionality reduction. Both methods tend to reduce the number of attributes in the dataset, but a dimensionality reduction method does so by creating new combinations of attributes (sometimes known as feature transformation), whereas feature selection methods include and exclude attributes present in the data without changing them.\n","\n","Some examples of dimensionality reduction methods are Principal Component Analysis, Singular Value Decomposition, Linear Discriminant Analysis, etc.\n","\n","Let me summarize the importance of feature selection for you:\n","\n","- It enables the machine learning algorithm to train faster.\n","- It reduces the complexity of a model and makes it easier to interpret.\n","- It improves the accuracy of a model if the right subset is chosen.\n","- It reduces Overfitting.\n","\n","In the next section, you will study the different types of general feature selection methods - Filter methods, Wrapper methods, and Embedded methods.\n","\n","## Filter methods\n","\n","The following image best describes filter-based feature selection methods:"]},{"attachments":{},"cell_type":"markdown","id":"b910d1b9-4f85-48e4-847a-422756c5a7af","metadata":{},"source":["![](graphics\\figure1.png)"]},{"attachments":{},"cell_type":"markdown","id":"e8df5266","metadata":{},"source":["<b>Image Source: Analytics Vidhya</b>\n","\n","Filter method relies on the general uniqueness of the data to be evaluated and pick feature subset, not including any mining\n","algorithm. Filter method uses the exact assessment criterion which includes distance, information, dependency, and consistency. The filter method uses the principal criteria of ranking technique and uses the rank ordering method for variable selection. The reason for using the ranking method is simplicity, produce excellent and relevant features. The ranking method will filter out irrelevant features before classification process starts.\n","\n","Filter methods are generally used as a data preprocessing step. The selection of features is independent of any machine learning algorithm. Features give rank on the basis of statistical scores which tend to determine the features' correlation with the outcome variable. Correlation is a heavily contextual term, and it varies from work to work. You can refer to the following table for defining correlation coefficients for different types of data (in this case continuous and categorical).\n","\n","![some text](graphics\\figure2.png)\n","\n","<b>Image Source: Analytics Vidhya</b>\n","\n","Some examples of some filter methods include the <b><i>Chi-squared test</i></b>, <b><i>information gain</i></b>, and <b><i>correlation coefficient scores</i></b>."]},{"attachments":{},"cell_type":"markdown","id":"9e394e0c","metadata":{},"source":["Next, you will see Wrapper methods.\n","\n","## Wrapper methods\n","\n","Like filter methods, let me give you a same kind of info-graphic which will help you to understand wrapper methods better:\n","\n","![some text](graphics\\figure3.png)\n","\n","<b>Image Source: Analytics Vidhya</b>\n","\n","As you can see in the above image, a wrapper method needs one machine learning algorithm and uses its performance as evaluation criteria. This method searches for a feature which is best-suited for the machine learning algorithm and aims to improve the mining performance. To evaluate the features, the predictive accuracy used for classification tasks and goodness of cluster is evaluated using clustering.\n","\n","Some typical examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n","\n","- **Forward Selection**: The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set. <br><br>\n","- **Backward Elimination**: The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set. <br><br>\n","- **Combination of forward selection and backward elimination**: The stepwise forward selection and backward elimination methods can be combined so that, at each step, the procedure selects the best attribute and removes the worst from among the remaining attributes. <br><br>\n","- **Recursive Feature elimination**: Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2<sup>N</sup> combinations of features.\n","\n","Good enough!\n","\n","Now let's study embedded methods.\n","\n","## Embedded methods\n","\n","Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n","\n","This is why Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n","\n","Examples of regularization algorithms are the <b>LASSO</b>, <b>Elastic Net</b>, <b>Ridge Regression</b>, etc.\n","\n","## Difference between filter and wrapper methods\n","\n","Well, it might get confusing at times to differentiate between filter methods and wrapper methods in terms of their functionalities. Let's take a look at what points they differ from each other.\n","\n","- Filter methods do not incorporate a machine learning model in order to determine if a feature is good or bad whereas wrapper methods use a machine learning model and train it the feature to decide if it is essential or not.\n","- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally costly, and in the case of massive datasets, wrapper methods are not the most effective feature selection method to consider.\n","- Filter methods may fail to find the best subset of features in situations when there is not enough data to model the statistical correlation of the features, but wrapper methods can always provide the best subset of features because of their exhaustive nature.\n","- Using features from wrapper methods in your final machine learning model can lead to overfitting as wrapper methods already train machine learning models with the features and it affects the true power of _learning_. But the features from filter methods will not lead to overfitting in most of the cases"]},{"attachments":{},"cell_type":"markdown","id":"2dd8e36c","metadata":{},"source":["So far you have studied the importance of feature selection, understood its difference with dimensionality reduction. You also covered various types of feature selection methods. So far, so good!\n","\n","Now, let's see some traps that you may get into while  performing feature selection:\n","\n","## Important consideration\n","\n","You may have already understood the worth of feature selection in a machine learning pipeline and the kind of services it provides if integrated. But it is very important to understand at exactly where you should integrate feature selection in your machine learning pipeline.\n","\n","Simply speaking, you should include the feature selection step before feeding the data to the model for training especially when you are using accuracy estimation methods such as _cross-validation_. This ensures that feature selection is performed on the data fold right before the model is trained. But if you perform feature selection first to prepare your data, then perform model selection and training on the selected features then it would be a blunder.\n","\n","If you perform feature selection on all of the data and then cross-validate, then the test data in each fold of the cross-validation procedure was also used to choose the features, and this tends to bias the performance of your machine learning model.\n","\n","Enough of theories! Let's get straight to some coding now.\n","\n","## A Case study in Python\n","\n","For this case study, you will use the Pima Indians Diabetes dataset. The description of the dataset can be found <a href = \"https://www.kaggle.com/uciml/pima-indians-diabetes-database\">here</a>.\n","\n","The dataset corresponds to classification tasks on which you need to predict if a person has diabetes based on 8 features.\n","\n","There are a total of 768 observations in the dataset. Your first task is to load the dataset so that you can proceed. But before that let's import the necessary dependencies, you are going to need. You can import the other ones as you go along.\n"]},{"cell_type":"code","execution_count":1,"id":"3ccca839","metadata":{"executionCancelledAt":null,"executionTime":152,"lastExecutedAt":1687521764242,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport numpy as np"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"attachments":{},"cell_type":"markdown","id":"c1af6cdc","metadata":{},"source":["Now that the dependencies are imported let's load the Pima Indians dataset into a Dataframe object with the help of Pandas library.\n"]},{"cell_type":"code","execution_count":2,"id":"abe56359","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1687521764290,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"data = pd.read_csv(\"data/diabetes.csv\")"},"outputs":[],"source":["data = pd.read_csv(\"_dataset\\diabetes.csv\")"]},{"attachments":{},"cell_type":"markdown","id":"ad4740de","metadata":{},"source":["The dataset is successfully loaded into the Dataframe object <i>data</i>. Now, let's take a look at the data.\n"]},{"cell_type":"code","execution_count":3,"id":"396d1604","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1687521764344,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"data.head()"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0            6      148             72             35        0  33.6   \n","1            1       85             66             29        0  26.6   \n","2            8      183             64              0        0  23.3   \n","3            1       89             66             23       94  28.1   \n","4            0      137             40             35      168  43.1   \n","\n","   DiabetesPedigreeFunction  Age  Outcome  \n","0                     0.627   50        1  \n","1                     0.351   31        0  \n","2                     0.672   32        1  \n","3                     0.167   21        0  \n","4                     2.288   33        1  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data.head()"]},{"attachments":{},"cell_type":"markdown","id":"14dc3feb","metadata":{},"source":["So you can see 8 different features labeled into the outcomes of 1 and 0 where 1 stands for the observation has diabetes, and 0 denotes the observation does not have diabetes. The dataset is known to have missing values. Specifically, there are missing observations for some columns that are marked as a zero value. You can deduce this by the definition of those columns, and it is impractical to have a zero value is invalid for those measures, e.g., zero for body mass index or blood pressure is invalid.\n","\n","But for this tutorial, you will directly use the preprocessed version of the dataset.\n"]},{"cell_type":"code","execution_count":4,"id":"0e5c5f98","metadata":{"executionCancelledAt":null,"executionTime":126,"lastExecutedAt":1687521764470,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# load data\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = pd.read_csv(url, names=names)"},"outputs":[],"source":["# load data\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","dataframe = pd.read_csv('_dataset\\pima-indians-diabetes.data.csv', names=names)\n"]},{"attachments":{},"cell_type":"markdown","id":"da036761","metadata":{},"source":["You loaded the data in a DataFrame object called <i>dataframe</i> now.\n","\n","Let's convert the DataFrame object to a NumPy array to achieve faster computation. Also, let's segregate the data into separate variables so that the features and the labels are separated.\n"]},{"cell_type":"code","execution_count":5,"id":"fb777c72","metadata":{"executionCancelledAt":null,"executionTime":45,"lastExecutedAt":1687521764517,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"array = dataframe.values\nX = array[:,0:8]\nY = array[:,8]"},"outputs":[],"source":["array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]"]},{"attachments":{},"cell_type":"markdown","id":"bfa9c016","metadata":{},"source":["Wonderful! You have prepared your data.\n","\n","First, you will implement a <b><i>Chi-Squared</i></b> statistical test for non-negative features to select 4 of the best features from the dataset. You have already seen Chi-Squared test belongs the class of filter methods. If anyone's curious about knowing the internals of Chi-Squared, <a href = \"https://www.youtube.com/watch?v=VskmMgXmkMQ\">this video</a> does an excellent job.\n","\n","The scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests to select a specific number of features, in this case, it is Chi-Squared.\n"]},{"cell_type":"code","execution_count":6,"id":"1cbeca27","metadata":{"executionCancelledAt":null,"executionTime":453,"lastExecutedAt":1687521764971,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the necessary libraries first\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2"},"outputs":[],"source":["# Import the necessary libraries first\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2"]},{"attachments":{},"cell_type":"markdown","id":"c3b3ea1a","metadata":{},"source":["You imported the libraries to run the experiments. Now, let's see it in action.\n"]},{"cell_type":"code","execution_count":7,"id":"d154a17e","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1687521765021,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Feature extraction\ntest = SelectKBest(score_func=chi2, k=4)\nfit = test.fit(X, Y)\n\n# Summarize scores\nnp.set_printoptions(precision=3)\nprint(fit.scores_)\n\nfeatures = fit.transform(X)\n# Summarize selected features\nprint(features[0:5,:])"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n","[[148.    0.   33.6  50. ]\n"," [ 85.    0.   26.6  31. ]\n"," [183.    0.   23.3  32. ]\n"," [ 89.   94.   28.1  21. ]\n"," [137.  168.   43.1  33. ]]\n"]}],"source":["# Feature extraction\n","test = SelectKBest(score_func=chi2, k=4)\n","fit = test.fit(X, Y)\n","\n","# Summarize scores\n","np.set_printoptions(precision=3)\n","print(fit.scores_)\n","\n","features = fit.transform(X)\n","# Summarize selected features\n","print(features[0:5,:])"]},{"attachments":{},"cell_type":"markdown","id":"5271b84c","metadata":{},"source":["### Interpretation:\n","You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): plas, test, mass, and age. This scores will help you further in determining the best features for training your model.\n","\n","<b>P.S.: The first row denotes the names of the features. For preprocessing of the dataset, the names have been numerically encoded.</b>\n","\n","Next, you will implement <b><i>Recursive Feature Elimination</i></b> which is a type of wrapper feature selection method.\n","\n","The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n","\n","It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n","\n","You can learn more about the `RFE` class in the <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\">scikit-learn documentation</a>.\n"]},{"cell_type":"code","execution_count":8,"id":"d7dfd189","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1687521765070,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import your necessary dependencies\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression"},"outputs":[],"source":["# Import your necessary dependencies\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression"]},{"attachments":{},"cell_type":"markdown","id":"abfa4f86","metadata":{},"source":["You will use RFE with the `Logistic Regression` classifier to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent.\n"]},{"cell_type":"code","execution_count":9,"id":"1d25c1dc","metadata":{"executionCancelledAt":null,"executionTime":68,"lastExecutedAt":1687521765138,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, n_features_to_select=3)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: %s\" % (fit.n_features_))\nprint(\"Selected Features: %s\" % (fit.support_))\nprint(\"Feature Ranking: %s\" % (fit.ranking_))"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num Features: 3\n","Selected Features: [ True False False False False  True  True False]\n","Feature Ranking: [1 2 4 6 5 1 1 3]\n"]}],"source":["# Feature extraction\n","model = LogisticRegression(max_iter=1000)\n","rfe = RFE(model, n_features_to_select=3)\n","fit = rfe.fit(X, Y)\n","print(\"Num Features: %s\" % (fit.n_features_))\n","print(\"Selected Features: %s\" % (fit.support_))\n","print(\"Feature Ranking: %s\" % (fit.ranking_))"]},{"attachments":{},"cell_type":"markdown","id":"db1a481e","metadata":{},"source":["You can see that RFE chose the top 3 features as `preg`, `mass`, and `pedi`.\n","\n","These are marked True in the <i>support_</i> array and marked with a choice “1” in the <i>ranking_</i> array. This, in turn, indicates the strength of these features.\n","\n","Next up you will use <b><i>Ridge regression</i></b> which is basically a regularization technique and an embedded feature selection techniques as well.\n","\n","<a href=\"https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/#three\">This article</a> gives you an excellent explanation on Ridge regression. Be sure to check it out.\n"]},{"cell_type":"code","execution_count":10,"id":"11ffa705","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1687521765185,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# First things first\nfrom sklearn.linear_model import Ridge"},"outputs":[],"source":["# First things first\n","from sklearn.linear_model import Ridge"]},{"attachments":{},"cell_type":"markdown","id":"b2b33942","metadata":{},"source":["Next, you will use Ridge regression to determine the coefficient R<sup>2</sup>.\n","\n","Also, <a href = \"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">check scikit-learn's official documentation on Ridge regression</a>.\n"]},{"cell_type":"code","execution_count":11,"id":"98e00030","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1687521765234,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"ridge = Ridge(alpha=1.0)\nridge.fit(X,Y)","lines_to_next_cell":0},"outputs":[{"data":{"text/plain":["Ridge()"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["ridge = Ridge(alpha=1.0)\n","ridge.fit(X,Y)"]},{"attachments":{},"cell_type":"markdown","id":"5eded530","metadata":{},"source":["In order to better understand the results of Ridge regression, you will implement a little helper function that will help you to print the results in a better so that you can interpret them easily.\n"]},{"cell_type":"code","execution_count":12,"id":"52723864","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1687521765285,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# A helper method for pretty-printing the coefficients\ndef pretty_print_coefs(coefs, names = None, sort = False):\n    if names == None:\n        names = [\"X%s\" % x for x in range(len(coefs))]\n    lst = zip(coefs, names)\n    if sort:\n        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n                                   for coef, name in lst)"},"outputs":[],"source":["# A helper method for pretty-printing the coefficients\n","def pretty_print_coefs(coefs, names = None, sort = False):\n","    if names == None:\n","        names = [\"X%s\" % x for x in range(len(coefs))]\n","    lst = zip(coefs, names)\n","    if sort:\n","        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n","    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n","                                   for coef, name in lst)"]},{"attachments":{},"cell_type":"markdown","id":"4b9ceea5","metadata":{},"source":["Next, you will pass Ridge model's coefficient terms to this little function and see what happens.\n"]},{"cell_type":"code","execution_count":13,"id":"31713159","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1687521765334,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print (\"Ridge model:\", pretty_print_coefs(ridge.coef_))"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ridge model: 0.021 * X0 + 0.006 * X1 + -0.002 * X2 + 0.0 * X3 + -0.0 * X4 + 0.013 * X5 + 0.145 * X6 + 0.003 * X7\n"]}],"source":["print (\"Ridge model:\", pretty_print_coefs(ridge.coef_))"]},{"attachments":{},"cell_type":"markdown","id":"965bae7d","metadata":{},"source":["You can spot all the coefficient terms appended with the feature variables. It will again help you to choose the most essential features. Below are some points that you should keep in mind while applying Ridge regression:\n","\n","- It is also known as <b>L2-Regularization</b>.\n","- For correlated features, it means that they tend to get similar coefficients.\n","- Feature having negative coefficients don't contribute that much. But in a more complex scenario where you are dealing with lots of features, then this score will definitely help you in the ultimate feature selection decision-making process.\n","\n","Well, that concludes the case study section. The methods that you implemented in the above section will help you to understand the features of a particular dataset in a comprehensive manner. Let me give you some critical points on these techniques:\n","\n","- Feature selection is essentially a part of data preprocessing which is considered to be the most time-consuming part of any machine learning pipeline.\n","- These techniques will help you to approach it in a more systematic way and machine learning friendly way. You will be able to interpret the features more accurately.\n","\n","## Wrap up!\n","In this post, you covered one of the most well studied and well researched statistical topics, i.e., feature selection. You also got familiar with its different variants and used them to see which features in a dataset are important.\n","\n","You can take this tutorial further by merging a correlation measure into the wrapper method and see how it performs. In the course of action, you might end up creating your own feature selection mechanism. That is how you establish the foundation for your little research. Researchers are also using various _soft computing_ principles in order to perform the selection. This is itself a whole field of study and research.  Also, you should try out the existing feature selection algorithms on various datasets and draw your own inferences.\n","\n","### Why do these traditional feature selection methods still hold? \n","\n","Yes, this question is obvious. Because there are neural net architectures (for example CNNs) which are quite capable of extracting the most significant features from data but that too has a limitation. Using a CNN for a regular tabular dataset which does not have specific properties (the properties that a typical image holds like transitional properties, edges, positional properties, contours etc.) is not the wisest decision to make. Moreover, when you have limited data and limited resources, training a CNN on regular tabular datasets might turn into a complete waste. So, in situations like that, the methods that you studied will definitely come handy.\n","\n","The following are some resources if you would like to dig more on this topic:\n","\n","- [Feature Selection for Knowledge Discovery and Data Mining](\"http://www.amazon.com/dp/079238198X?tag=inspiredalgor-20\")\n","- [Subspace, Latent Structure, and Feature Selection: Statistical and Optimization Perspectives Workshop](\"http://www.amazon.com/dp/3540341374?tag=inspiredalgor-20\")\n","- [Feature Selection: Problem statement and Uses](\"https://www.youtube.com/watch?v=y2Jsa4sgD5w&t=1073s\")\n","- [Using genetic algorithms for feature selection in Data Analytics](\"https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection\")\n","\n","Below are the references that were used in order to write this tutorial.\n","\n","- <a href = \"https://www.elsevier.com/books/data-mining-concepts-and-techniques/han/978-0-12-381479-1\">Data Mining: Concepts and Techniques; Jiawei Han Micheline Kamber Jian Pei</a>.\n","- <a href = \"http://machinelearningmastery.com/an-introduction-to-feature-selection/\">An introduction to feature selection</a>\n","- [Analytics Vidhya article on feature selection](\"https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/\")\n","- <a href = \"https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models\">Hierarchical and Mixed Model - DataCamp course</a>\n","- [Feature Selection For Machine Learning in Python](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n","- [Outlier Detection in Stream Data by Machine Learning and Feature Selection Methods](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.428.6930&rep=rep1&type=pdf)\n","- S. Visalakshi and V. Radha, \"A literature review of feature selection techniques and applications: Review of feature selection in data mining,\" 2014 IEEE International Conference on Computational Intelligence and Computing Research, Coimbatore, 2014, pp. 1-6.\n"]}],"metadata":{"editor":"DataCamp Workspace","jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}
